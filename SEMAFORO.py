# -*- coding: utf-8 -*-
"""SemaforoInteligente.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JuXde2CFJD6xL54rC65R3AHzwiTp8jdH

# DETECCION DE OBJETOS

CREACION Y ENTRENAMIENTO DE DE UN MODELO CON TORCHVISION PARA GENERAR DETECCIONES  Y SE UTILIZA UN DATASET MODELO Y CONOCIDO COCO
"""



import torch
import torchvision

train = torchvision.datasets.VOCDetection('./data', download=True) # Cargar el conjunto de datos VOC Detection
len(train)

import torch  # Importación de librerías
import torchvision
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
import matplotlib.patheffects as PathEffects
import random

device = "cuda" if torch.cuda.is_available() else "cpu" # Verificar si hay una GPU disponible

train = torchvision.datasets.VOCDetection('./data', download=True) # Cargar el conjunto de datos VOC Detection

voc_classes = ["background", "bicycle", "bus", "car", "motorbike", "person"] # Definir una lista de las clases de objetos en el conjunto de datos

def get_sample(ix):
    img, label = train[ix] # Cargar la imagen y las anotaciones correspondientes
    img_np = np.array(img)  # Convertir la imagen en un arreglo NumPy
    anns = label['annotation']['object']  # Extraer las anotaciones de la variable label
    if type(anns) is not list: # Si hay más de una anotación, almacenarlas en una lista
        anns = [anns]
    labels = np.array([voc_classes.index(ann['name']) for ann in anns if ann['name'] in voc_classes]) # Crear una lista de etiquetas y una lista de cuadros delimitadores
                                                                                                      # (bounding boxes) para cada objeto en la imagen
    bbs = [ann['bndbox'] for ann in anns if ann['name'] in voc_classes]
    bbs = np.array([[int(bb['xmin']), int(bb['ymin']), int(bb['xmax'])-int(bb['xmin']), int(bb['ymax'])-int(bb['ymin'])] for bb in bbs])
    anns = (labels, bbs) # Devolver la imagen como un arreglo NumPy y las anotaciones como una tupla que contiene las etiquetas y los cuadros delimitadores
    return img_np, anns

def plot_anns(img, anns, ax=None, bg=-1, classes=voc_classes):
    if not ax: # Crear una nueva figura y un nuevo objeto de eje si no se proporcionó uno
        fig, ax = plt.subplots(figsize=(10, 6))
    ax.imshow(img) # Mostrar la imagen en el objeto de eje
    labels, bbs = anns # Extraer las etiquetas y los cuadros delimitadores de las anotaciones y recorrerlos en un bucle
    for lab, bb in zip(labels, bbs):
        if bg == -1 or lab != bg: # Si se proporcionó un fondo (background) y la etiqueta es igual al fondo, omitir el objeto
            x, y, w, h = bb
            rect = mpatches.Rectangle((x, y), w, h, fill=False, edgecolor='red', linewidth=2) # Crear un rectángulo rojo que representa el cuadro delimitador del objeto
            text = ax.text(x, y - 10, classes[lab], {'color': 'red'})
            text.set_path_effects([PathEffects.withStroke(linewidth=5, foreground='w')])
            ax.add_patch(rect) # Agregar el rectángulo al objeto de eje

r, c = 3, 4 # Definir el número de filas y columnas
fig = plt.figure(figsize=(4*c, 4*r)) # Crear la figura con un tamaño de 4 veces el número de columnas y filas
for _r in range(r): # Iterar a través de cada fila
    for _c in range(c): # Iterar a través de cada columna
        ax = plt.subplot(r, c, _r*c + _c + 1) # Crear un eje para la imagen actual
        ix = random.randint(0, len(train)-1) # Obtener un índice aleatorio para una imagen del conjunto de datos
        img_np, anns = get_sample(ix) # Obtener la imagen y las anotaciones correspondientes al índice aleatorio
        plot_anns(img_np, anns, ax) # Dibujar la imagen en el eje actual
        plt.axis("off") # Desactivar los ejes

plt.tight_layout() # Ajustar el diseño de la figura para que se ajuste a las imágenes
plt.show() # Mostrar la figura

model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)
model

img_np, anns = get_sample(4523)
plot_anns(img_np, anns)
plt.show()

model.eval()
outputs = model([torch.tensor(img_np / 255.).permute(2,0,1).float()])
outputs

COCO_INSTANCE_CATEGORY_NAMES = ["background",
            "bicycle",
            "bus",
            "car",
            "motorbike",
            "person"]

def predict(img, threshold=0.8):
    model.eval()
    outputs = model([torch.tensor(img_np / 255.).permute(2,0,1).float()])
    # nos quedamos con la primera detección
    bb = outputs[0]['boxes'][0].long().tolist()
    bbs = [[bb[0], bb[1], bb[2]-bb[0], bb[3]-bb[1]] for o in outputs for bb, score in zip(o['boxes'], o['scores']) if score > threshold]
    labels = [lab for o in outputs for lab, score in zip(o['labels'], o['scores']) if score > threshold]
    return labels, bbs

ix = random.randint(0, len(train)-1)
img_np, anns = get_sample(ix)
plot_anns(img_np, anns)
plt.show()

import cv2
import torch
from google.colab.patches import cv2_imshow
# Cargar el modelo preentrenado Faster R-CNN
model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)
model.eval()

# Definir las clases de objetos
COCO_INSTANCE_CATEGORY_NAMES = ["background", "bicycle", "bus", "car", "motorbike", "person", "truck"]


# Función para predecir las detecciones en una imagen
def predict(img, threshold=0.8):
    outputs = model([img[0]])
    detections = []
    for output in outputs:
        for box, label, score in zip(output['boxes'], output['labels'], output['scores']):
            if score > threshold and COCO_INSTANCE_CATEGORY_NAMES[label] == "car":
                bbox = box.tolist()
                detections.append([bbox[0], bbox[1], bbox[2] - bbox[0], bbox[3] - bbox[1]])
    return detections

# Leer el video
video_path = "/content/pexels-christopher-schultz-5927708-1080x1920-30fps.mp4"
cap = cv2.VideoCapture(video_path)

# Inicializar el contador de coches
car_count = 0

def controlar_semaforos(preferencia):
    if preferencia == "preferencia":
        # Lógica para dar preferencia al otro video con más coches
        # Controlar los semáforos según la preferencia
        print("Dar preferencia al otro video con más coches")
    else:
        # Lógica para dar preferencia al video actual con menos coches
        # Controlar los semáforos según la preferencia
        print("Dar preferencia al video actual con menos coches")


while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    # Preprocesar la imagen
    img = torch.from_numpy(frame / 255.0).permute(2, 0, 1).float().unsqueeze(0)

    # Obtener las detecciones de coches en la imagen
    detections = predict(img)

    # Actualizar el contador de coches
    car_count += len(detections)

    # Controlar los semáforos según el contador de coches
    if car_count > 5:
        # Dar preferencia al carril actual con más coches
        controlar_semaforos("preferencia")
    else:
        # Dar preferencia al carril actual con menos coches
        controlar_semaforos("preferencia")

    # Mostrar la imagen con las detecciones
    for bbox in detections:
        x, y, w, h = bbox
        x, y, w, h = int(x), int(y), int(w), int(h)  # Convertir las coordenadas a enteros
        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)
    cv2_imshow(frame)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()

from google.colab import drive
drive.mount('/content/drive')

import torch.nn as nn

class MyModel(nn.Module):
    def __init__(self):
        super(MyModel, self).__init__()
        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)
        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)
        self.fc1 = nn.Linear(32 * 7 * 7, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 6)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(-1, 32 * 7 * 7)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

    def predict(self, x):
        return x

import torch
import torchvision
from torch.utils.data import DataLoader
from sklearn.metrics import accuracy_score

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

train_transforms = torchvision.transforms.Compose([
    torchvision.transforms.Resize((224, 224)),
    torchvision.transforms.ToTensor(),
])

train = torchvision.datasets.VOCDetection('./data', download=True, transform=train_transforms)

test_transforms = torchvision.transforms.Compose([
    torchvision.transforms.Resize((224, 224)),
    torchvision.transforms.ToTensor(),
])

test_dataset = torchvision.datasets.VOCDetection('./data', download=True, transform=test_transforms)
test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)

voc_classes = ["background", "bicycle", "bus", "car", "motorbike", "person"]


model = MyModel()

import numpy as np
y_true = []
y_pred = []

for image_batch, label_batch in test_loader:
    image_batch = image_batch[0].to(device)
    label_batch = label_batch['annotation']['object'][0]['bndbox']['xmin']

    y_true.append(label_batch)
    y_pred.append(np.argmax(model.predict(image_batch), axis=-1))

y_true = np.concatenate(y_true)
y_pred = np.concatenate(y_pred)

y_true_str = []
y_pred_str = []

for i in range(len(y_true)):
    if int(y_true[i]) >= len(voc_classes):
        y_true_str.append('unknown')
    else:
        y_true_str.append(voc_classes[int(y_true[i])])

    if int(y_pred[i][0]) >= len(voc_classes):
        y_pred_str.append('unknown')
    else:
        y_pred_str.append(voc_classes[int(y_pred[i][0])])

accuracy = accuracy_score(y_true_str, y_pred_str)
print('Accuracy:', accuracy)

from sklearn.metrics import f1_score
y_true = []
y_pred = []

for image_batch, label_batch in test_loader:
    image_batch = image_batch[0].to(device)
    label_batch = label_batch['annotation']['object'][0]['bndbox']['xmin']

    y_true.append(label_batch)
    y_pred.append(np.argmax(model.predict(image_batch), axis=-1))

y_true = np.concatenate(y_true)
y_pred = np.concatenate(y_pred)

y_true_str = []
y_pred_str = []

for i in range(len(y_true)):
    if int(y_true[i]) >= len(voc_classes):
        y_true_str.append('unknown')
    else:
        y_true_str.append(voc_classes[int(y_true[i])])

    if int(y_pred[i][0]) >= len(voc_classes):
        y_pred_str.append('unknown')
    else:
        y_pred_str.append(voc_classes[int(y_pred[i][0])])

f1_score = f1_score(y_true_str, y_pred_str, average='weighted')
print('F1 score:', f1_score)

from sklearn.metrics import confusion_matrix
y_true = []
y_pred = []

for image_batch, label_batch in test_loader:
    image_batch = image_batch[0].to(device)
    label_batch = label_batch['annotation']['object'][0]['bndbox']['xmin']

    y_true.append(label_batch)
    y_pred.append(np.argmax(model.predict(image_batch), axis=-1))

y_true = np.concatenate(y_true)
y_pred = np.concatenate(y_pred)

y_true_str = []
y_pred_str = []

for i in range(len(y_true)):
    if int(y_true[i]) >= len(voc_classes):
        y_true_str.append('unknown')
    else:
        y_true_str.append(voc_classes[int(y_true[i])])

    if int(y_pred[i][0]) >= len(voc_classes):
        y_pred_str.append('unknown')
    else:
        y_pred_str.append(voc_classes[int(y_pred[i][0])])

confusion_matrix = confusion_matrix(y_true_str, y_pred_str)
print('Confusion matrix:\n', confusion_matrix)

from sklearn.metrics import recall_score

y_true = []
y_pred = []

for image_batch, label_batch in test_loader:
    image_batch = image_batch[0].to(device)
    label_batch = label_batch['annotation']['object'][0]['bndbox']['xmin']

    y_true.append(label_batch)
    y_pred.append(np.argmax(model.predict(image_batch), axis=-1))

y_true = np.concatenate(y_true)
y_pred = np.concatenate(y_pred)

y_true_str = []
y_pred_str = []

for i in range(len(y_true)):
    if int(y_true[i]) >= len(voc_classes):
        y_true_str.append('unknown')
    else:
        y_true_str.append(voc_classes[int(y_true[i])])

    if int(y_pred[i][0]) >= len(voc_classes):
        y_pred_str.append('unknown')
    else:
        y_pred_str.append(voc_classes[int(y_pred[i][0])])

recall_score = recall_score(y_true_str, y_pred_str, average='weighted', zero_division=0)
print('Recall score:', recall_score)

from sklearn.metrics import log_loss
from sklearn.preprocessing import LabelEncoder, OneHotEncoder

voc_classes.append('unknown')

y_true = []
y_pred = []

for image_batch, label_batch in test_loader:
    image_batch = image_batch[0].to(device)
    label_batch = label_batch['annotation']['object'][0]['bndbox']['xmin']

    y_true.append(label_batch)
    y_pred.append(np.argmax(model.predict(image_batch), axis=-1))

y_true = np.concatenate(y_true)
y_pred = np.concatenate(y_pred)

le = LabelEncoder()
le.fit(voc_classes)

y_true_str = []
y_pred_str = []

for i in range(len(y_true)):
    if int(y_true[i]) >= len(voc_classes):
        y_true_str.append('unknown')
    else:
        y_true_str.append(voc_classes[int(y_true[i])])

    if int(y_pred[i][0]) >= len(voc_classes):
        y_pred_str.append('unknown')
    else:
        y_pred_str.append(voc_classes[int(y_pred[i][0])])

y_true_num = le.transform(y_true_str)
y_pred_num = le.transform(y_pred_str)

ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
y_true_ohe = ohe.fit_transform(y_true_num.reshape(-1, 1))
y_pred_ohe = ohe.transform(y_pred_num.reshape(-1, 1))

log_loss = log_loss(y_true_ohe, y_pred_ohe)
print('Logarithmic loss:', log_loss)
