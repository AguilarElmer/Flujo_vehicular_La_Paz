# -*- coding: utf-8 -*-
"""Untitled7.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JuXde2CFJD6xL54rC65R3AHzwiTp8jdH

# DETECCION DE OBJETOS

CREACION Y ENTRENAMIENTO DE DE UN MODELO CON TORCHVISION PARA GENERAR DETECCIONES  Y SE UTILIZA UN DATASET MODELO Y CONOCIDO COCO
"""



import torch
import torchvision
device = "cuda" if torch.cuda.is_available() else "cpu" # Verificar si hay una GPU disponible
device

train = torchvision.datasets.VOCDetection('./data', download=True) # Cargar el conjunto de datos VOC Detection
len(train)

voc_classes = ["background",
            "bicycle",
            "bird",
            "bus",
            "car",
            "motorbike",
            "person"]  # Lista de las clases de objetos en el conjunto de datos VOC Detection

import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
import matplotlib.patheffects as PathEffects
import random

device = "cuda" if torch.cuda.is_available() else "cpu" # Verificar si hay una GPU disponible

train = torchvision.datasets.VOCDetection('./data', download=True) # Cargar el conjunto de datos VOC Detection

voc_classes = ["background", "bicycle", "bus", "car", "motorbike", "person"] # Definir una lista de las clases de objetos en el conjunto de datos

def get_sample(ix):
    img, label = train[ix] # Cargar la imagen y las anotaciones correspondientes
    img_np = np.array(img)  # Convertir la imagen en un arreglo NumPy
    anns = label['annotation']['object']  # Extraer las anotaciones de la variable label
    if type(anns) is not list: # Si hay más de una anotación, almacenarlas en una lista
        anns = [anns]
    labels = np.array([voc_classes.index(ann['name']) for ann in anns if ann['name'] in voc_classes]) # Crear una lista de etiquetas y una lista de cuadros delimitadores
                                                                                                      # (bounding boxes) para cada objeto en la imagen
    bbs = [ann['bndbox'] for ann in anns if ann['name'] in voc_classes]
    bbs = np.array([[int(bb['xmin']), int(bb['ymin']), int(bb['xmax'])-int(bb['xmin']), int(bb['ymax'])-int(bb['ymin'])] for bb in bbs])
    anns = (labels, bbs) # Devolver la imagen como un arreglo NumPy y las anotaciones como una tupla que contiene las etiquetas y los cuadros delimitadores
    return img_np, anns

def plot_anns(img, anns, ax=None, bg=-1, classes=voc_classes):
    if not ax: # Crear una nueva figura y un nuevo objeto de eje si no se proporcionó uno
        fig, ax = plt.subplots(figsize=(10, 6))
    ax.imshow(img) # Mostrar la imagen en el objeto de eje
    labels, bbs = anns # Extraer las etiquetas y los cuadros delimitadores de las anotaciones y recorrerlos en un bucle
    for lab, bb in zip(labels, bbs):
        if bg == -1 or lab != bg: # Si se proporcionó un fondo (background) y la etiqueta es igual al fondo, omitir el objeto
            x, y, w, h = bb
            rect = mpatches.Rectangle((x, y), w, h, fill=False, edgecolor='red', linewidth=2) # Crear un rectángulo rojo que representa el cuadro delimitador del objeto
            text = ax.text(x, y - 10, classes[lab], {'color': 'red'})
            text.set_path_effects([PathEffects.withStroke(linewidth=5, foreground='w')])
            ax.add_patch(rect) # Agregar el rectángulo al objeto de eje

r, c = 3, 4 # Definir el número de filas y columnas
fig = plt.figure(figsize=(4*c, 4*r)) # Crear la figura con un tamaño de 4 veces el número de columnas y filas
for _r in range(r): # Iterar a través de cada fila
    for _c in range(c): # Iterar a través de cada columna
        ax = plt.subplot(r, c, _r*c + _c + 1) # Crear un eje para la imagen actual
        ix = random.randint(0, len(train)-1) # Obtener un índice aleatorio para una imagen del conjunto de datos
        img_np, anns = get_sample(ix) # Obtener la imagen y las anotaciones correspondientes al índice aleatorio
        plot_anns(img_np, anns, ax) # Dibujar la imagen en el eje actual
        plt.axis("off") # Desactivar los ejes

plt.tight_layout() # Ajustar el diseño de la figura para que se ajuste a las imágenes
plt.show() # Mostrar la figura

model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)
model

img_np, anns = get_sample(4523)
plot_anns(img_np, anns)
plt.show()

model.eval()
outputs = model([torch.tensor(img_np / 255.).permute(2,0,1).float()])
outputs

COCO_INSTANCE_CATEGORY_NAMES = ["background",
            "bicycle",
            "bird",
            "bus",
            "car",
            "motorbike",
            "person"]

def predict(img, threshold=0.8):
    model.eval()
    outputs = model([torch.tensor(img_np / 255.).permute(2,0,1).float()])
    # nos quedamos con la primera detección
    bb = outputs[0]['boxes'][0].long().tolist()
    bbs = [[bb[0], bb[1], bb[2]-bb[0], bb[3]-bb[1]] for o in outputs for bb, score in zip(o['boxes'], o['scores']) if score > threshold]
    labels = [lab for o in outputs for lab, score in zip(o['labels'], o['scores']) if score > threshold]
    return labels, bbs

ix = random.randint(0, len(train)-1)
img_np, anns = get_sample(ix)
plot_anns(img_np, anns)
plt.show()

import cv2
import torch
from google.colab.patches import cv2_imshow
# Cargar el modelo preentrenado Faster R-CNN
model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)
model.eval()

# Definir las clases de objetos
COCO_INSTANCE_CATEGORY_NAMES = ["background", "bicycle", "bird", "bus", "car", "motorbike", "person"]

# Función para predecir las detecciones en una imagen
def predict(img, threshold=0.8):
    outputs = model([img[0]])
    detections = []
    for output in outputs:
        for box, label, score in zip(output['boxes'], output['labels'], output['scores']):
            if score > threshold and COCO_INSTANCE_CATEGORY_NAMES[label] == "car":
                bbox = box.tolist()
                detections.append([bbox[0], bbox[1], bbox[2] - bbox[0], bbox[3] - bbox[1]])
    return detections

# Leer el video
video_path = "/content/pexels-christopher-schultz-5927708-1080x1920-30fps.mp4"
cap = cv2.VideoCapture(video_path)

# Inicializar el contador de coches
car_count = 0

def controlar_semaforos(preferencia):
    if preferencia == "preferencia":
        # Lógica para dar preferencia al otro video con más coches
        # Controlar los semáforos según la preferencia
        print("Dar preferencia al otro video con más coches")
    else:
        # Lógica para dar preferencia al video actual con menos coches
        # Controlar los semáforos según la preferencia
        print("Dar preferencia al video actual con menos coches")


while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    # Preprocesar la imagen
    img = torch.from_numpy(frame / 255.0).permute(2, 0, 1).float().unsqueeze(0)

    # Obtener las detecciones de coches en la imagen
    detections = predict(img)

    # Actualizar el contador de coches
    car_count += len(detections)

    # Controlar los semáforos según el contador de coches
    if car_count > 5:
        # Dar preferencia al carril actual con más coches
        controlar_semaforos("preferencia")
    else:
        # Dar preferencia al carril actual con menos coches
        controlar_semaforos("preferencia")

    # Mostrar la imagen con las detecciones
    for bbox in detections:
        x, y, w, h = bbox
        x, y, w, h = int(x), int(y), int(w), int(h)  # Convertir las coordenadas a enteros
        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)
    cv2_imshow(frame)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()
